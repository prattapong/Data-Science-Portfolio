{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "try:\n",
    "    import holidays\n",
    "except:\n",
    "    !pip install holidays\n",
    "    import holidays\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except:\n",
    "    !pip install xgboost\n",
    "    import xgboost as xgb\n",
    "try:\n",
    "    from lunardate import LunarDate\n",
    "except:\n",
    "    !pip install lunardate\n",
    "    from lunardate import LunarDate\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.fft import fft\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define url path to CSV\n",
    "annex1 = 'https://raw.githubusercontent.com/prattapong/Data-Science-Portfolio/main/Projects/Supermaket%20Sales%20Data/data/annex1.csv'\n",
    "annex2 = 'https://raw.githubusercontent.com/prattapong/Data-Science-Portfolio/main/Projects/Supermaket%20Sales%20Data/data/annex2.csv'\n",
    "annex3 = 'https://raw.githubusercontent.com/prattapong/Data-Science-Portfolio/main/Projects/Supermaket%20Sales%20Data/data/annex3.csv'\n",
    "annex4 = 'https://raw.githubusercontent.com/prattapong/Data-Science-Portfolio/main/Projects/Supermaket%20Sales%20Data/data/annex4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_url(url):\n",
    "    # Create request response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Create a file-like object from the raw content\n",
    "        csv_content = StringIO(response.text)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_content)\n",
    "\n",
    "        # Return DataFrame\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"Failed to fetch data. Status code: {response.status_code}\")\n",
    "\n",
    "df_item = get_df_from_url(annex1)\n",
    "df_transaction = get_df_from_url(annex2)\n",
    "df_wholesale = get_df_from_url(annex3)\n",
    "df_loss = get_df_from_url(annex4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merge and aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_df(df_transaction: pd.DataFrame = df_transaction,\n",
    "                 df_item: pd.DataFrame = df_item,\n",
    "                 df_wholesale: pd.DataFrame = df_wholesale,\n",
    "                 df_loss: pd.DataFrame = df_loss):\n",
    "\n",
    "    df_merge = df_transaction.merge(df_item,\n",
    "                                    how = 'left',\n",
    "                                    on = 'Item Code')\n",
    "    df_merge = df_merge.merge(df_wholesale,\n",
    "                            how = 'left',\n",
    "                            on = ['Date', 'Item Code'])\n",
    "    df_merge = df_merge.merge(df_loss.drop('Item Name', axis = 1),\n",
    "                            how = 'left',\n",
    "                            on = 'Item Code')\n",
    "    \n",
    "    return df_merge\n",
    "\n",
    "df_merge = merge_all_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Category Code</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1011010101</td>\n",
       "      <td>1503.7896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1011010201</td>\n",
       "      <td>592.5300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1011010402</td>\n",
       "      <td>70.2838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1011010501</td>\n",
       "      <td>176.8180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>1011010504</td>\n",
       "      <td>759.9902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Category Code      Sales\n",
       "0  2020-07-01     1011010101  1503.7896\n",
       "1  2020-07-01     1011010201   592.5300\n",
       "2  2020-07-01     1011010402    70.2838\n",
       "3  2020-07-01     1011010501   176.8180\n",
       "4  2020-07-01     1011010504   759.9902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Sales column\n",
    "df_merge['Sales'] = df_merge['Unit Selling Price (RMB/kg)'] * df_merge['Quantity Sold (kilo)']\n",
    "\n",
    "# Aggregate as new table\n",
    "df_agg = df_merge.groupby(['Date', 'Category Code'], as_index = False)['Sales'].sum()\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_feature(df: pd.DataFrame,\n",
    "                       date_column: str):\n",
    "    # Convert the 'Date' column to datetime type\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "\n",
    "    # Extract date components\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['Day'] = df['Date'].dt.day\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek  # Monday is 0 and Sunday is 6\n",
    "    df['WeekNumber'] = df['Date'].dt.isocalendar().week\n",
    "    df['Quarter'] = df['Date'].dt.quarter\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_holiday(date_value):\n",
    "    china_holidays = holidays.country_holidays('CN')\n",
    "    value = 1 if date_value in china_holidays else 0\n",
    "    return value\n",
    "\n",
    "def create_lag_feature(df: pd.DataFrame,\n",
    "                       days: int):\n",
    "    \n",
    "    df[f'lag_{days}'] = df['Sales'].shift(days)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_chinese_new_year_dates(start_year, end_year):\n",
    "    chinese_new_year_dates = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        lunar_date = LunarDate(year, 1, 1)\n",
    "        cny_date = pd.Timestamp(lunar_date.toSolarDate())\n",
    "\n",
    "        chinese_new_year_dates.append({'Chinese New Year Date': cny_date})\n",
    "\n",
    "    chinese_new_year_df = pd.DataFrame(chinese_new_year_dates)\n",
    "    chinese_new_year_df['Last Chinese New Year Date'] = chinese_new_year_df['Chinese New Year Date'].shift(1)\n",
    "    chinese_new_year_df.dropna(subset=['Last Chinese New Year Date'], inplace = True)\n",
    "    \n",
    "    return chinese_new_year_df\n",
    "\n",
    "def get_chinese_new_year_period(df, days):\n",
    "    days = 7\n",
    "    # new_years = df['Chinese New Year Date'].unique()\n",
    "    # for new_year in new_years:\n",
    "    new_year_dict = dict(zip(df['Chinese New Year Date'], df['Last Chinese New Year Date']))\n",
    "    for this_year, last_year in new_year_dict.items():\n",
    "        for day in range(-days, days + 1):\n",
    "            if day != 0:\n",
    "                df = pd.concat([df, pd.DataFrame({'Chinese New Year Date': [this_year + timedelta(days = day)],\n",
    "                                                  'Last Chinese New Year Date': [last_year + timedelta(days = day)]})],\n",
    "                               axis = 0,\n",
    "                               ignore_index = True)\n",
    "    return df\n",
    "\n",
    "def create_last_new_year_feature(df: pd.DataFrame,\n",
    "                                 days_before_after: int):\n",
    "    \n",
    "    # Get start and end year to generate Chinese New Year DataFrame from input data\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    start_year = df['Date'].min().year\n",
    "    end_year = df['Date'].max().year\n",
    "    \n",
    "    # Get Chinese New Year DataFrame\n",
    "    df_chinese_new_year = get_chinese_new_year_dates(start_year = start_year,\n",
    "                                                     end_year = end_year)\n",
    "    df_chinese_new_year_period = get_chinese_new_year_period(df = df_chinese_new_year,\n",
    "                                                             days = 7)\n",
    "\n",
    "    # Cast date column\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Merge current new year date with last new year date\n",
    "    df_last_new_year = df.copy()\n",
    "    df_last_new_year = df_last_new_year.merge(df_chinese_new_year_period,\n",
    "                                              how = 'left',\n",
    "                                              left_on = 'Date',\n",
    "                                              right_on = 'Last Chinese New Year Date')\n",
    "    # Cleanup DataFrame\n",
    "    df_last_new_year = df_last_new_year[~df_last_new_year['Last Chinese New Year Date'].isna()][['Chinese New Year Date', 'Category Code', 'Sales']]\n",
    "\n",
    "    # Change column name before merge avoiding duplicated columns\n",
    "    df_last_new_year.columns = ['Date', 'Category Code', 'Last New Year Sales']\n",
    "\n",
    "    # Merge current new year with current new year to get \"Last New Year Sales\"\n",
    "    df = df.merge(df_last_new_year,\n",
    "                  on = ['Date', 'Category Code'],\n",
    "                  how = 'left')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_feature_engineering(df_category: pd.DataFrame):\n",
    "    df_category = create_date_feature(df_category,\n",
    "                                    date_column = 'Date')\n",
    "    df_category = create_lag_feature(df = df_category,\n",
    "                                    days = 364)\n",
    "    df_category = create_lag_feature(df = df_category,\n",
    "                                    days = 7)\n",
    "    df_category = create_lag_feature(df = df_category,\n",
    "                                    days = 14)\n",
    "    df_category = create_lag_feature(df = df_category,\n",
    "                                    days = 28)\n",
    "    df_category = create_last_new_year_feature(df = df_category,\n",
    "                                            days_before_after = 7)\n",
    "    df_category['is_holiday'] = df_category['Date'].apply(is_holiday)\n",
    "\n",
    "    df_final = df_category.drop(['Date', 'Category Code'], axis = 1)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train and evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Loop Category and Cross Validition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010101 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [1:03:20<00:00, 13.19s/combination] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010101 Best Parameters: {'n_estimators': 5000, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.75, 'colsample_bytree': 0.5, 'early_stopping_rounds': 200}\n",
      "Category 1011010101 Best Score: 309.0581559119968\n",
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010201 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [1:06:03<00:00, 13.76s/combination]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010201 Best Parameters: {'n_estimators': 10000, 'learning_rate': 0.001, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.5, 'early_stopping_rounds': 400}\n",
      "Category 1011010201 Best Score: 156.40109678989998\n",
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010402 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [50:22<00:00, 10.50s/combination]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010402 Best Parameters: {'n_estimators': 5000, 'learning_rate': 0.001, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.5, 'early_stopping_rounds': 100}\n",
      "Category 1011010402 Best Score: 216.2977724413237\n",
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010501 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [28:40<00:00,  5.98s/combination]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010501 Best Parameters: {'n_estimators': 5000, 'learning_rate': 0.001, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.5, 'early_stopping_rounds': 200}\n",
      "Category 1011010501 Best Score: 100.29919163584293\n",
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010504 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [32:17<00:00,  6.73s/combination]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010504 Best Parameters: {'n_estimators': 1000, 'learning_rate': 0.01, 'max_depth': 3, 'subsample': 0.75, 'colsample_bytree': 1.0, 'early_stopping_rounds': 100}\n",
      "Category 1011010504 Best Score: 408.6057863668257\n",
      "\n",
      "######################################################\n",
      "################ CATEGORY: 1011010801 ################\n",
      "######################################################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|██████████| 288/288 [35:36<00:00,  7.42s/combination]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Category 1011010801 Best Parameters: {'n_estimators': 10000, 'learning_rate': 0.001, 'max_depth': 3, 'subsample': 0.5, 'colsample_bytree': 0.5, 'early_stopping_rounds': 100}\n",
      "Category 1011010801 Best Score: 265.78633975804\n",
      "\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SUMMARY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1011010101</th>\n",
       "      <th>1011010201</th>\n",
       "      <th>1011010402</th>\n",
       "      <th>1011010501</th>\n",
       "      <th>1011010504</th>\n",
       "      <th>1011010801</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_estimators</th>\n",
       "      <td>5000.00</td>\n",
       "      <td>10000.000</td>\n",
       "      <td>5000.000</td>\n",
       "      <td>5000.000</td>\n",
       "      <td>1000.00</td>\n",
       "      <td>10000.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_depth</th>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subsample</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colsample_bytree</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.500</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_stopping_rounds</th>\n",
       "      <td>200.00</td>\n",
       "      <td>400.000</td>\n",
       "      <td>100.000</td>\n",
       "      <td>200.000</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       1011010101  1011010201  1011010402  1011010501  \\\n",
       "n_estimators              5000.00   10000.000    5000.000    5000.000   \n",
       "learning_rate                0.01       0.001       0.001       0.001   \n",
       "max_depth                    3.00       3.000       3.000       3.000   \n",
       "subsample                    0.75       0.500       0.500       0.500   \n",
       "colsample_bytree             0.50       0.500       0.500       0.500   \n",
       "early_stopping_rounds      200.00     400.000     100.000     200.000   \n",
       "\n",
       "                       1011010504  1011010801  \n",
       "n_estimators              1000.00   10000.000  \n",
       "learning_rate                0.01       0.001  \n",
       "max_depth                    3.00       3.000  \n",
       "subsample                    0.75       0.500  \n",
       "colsample_bytree             1.00       0.500  \n",
       "early_stopping_rounds      100.00     100.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Normalized RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011010101</td>\n",
       "      <td>309.058156</td>\n",
       "      <td>0.112941</td>\n",
       "      <td>0.270468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1011010201</td>\n",
       "      <td>156.401097</td>\n",
       "      <td>0.136814</td>\n",
       "      <td>0.620382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1011010402</td>\n",
       "      <td>216.297772</td>\n",
       "      <td>0.123281</td>\n",
       "      <td>0.685395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1011010501</td>\n",
       "      <td>100.299192</td>\n",
       "      <td>0.113782</td>\n",
       "      <td>0.654450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1011010504</td>\n",
       "      <td>408.605786</td>\n",
       "      <td>0.102521</td>\n",
       "      <td>0.399783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1011010801</td>\n",
       "      <td>265.786340</td>\n",
       "      <td>0.121651</td>\n",
       "      <td>0.430013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category        RMSE  Normalized RMSE      MAPE\n",
       "0  1011010101  309.058156         0.112941  0.270468\n",
       "1  1011010201  156.401097         0.136814  0.620382\n",
       "2  1011010402  216.297772         0.123281  0.685395\n",
       "3  1011010501  100.299192         0.113782  0.654450\n",
       "4  1011010504  408.605786         0.102521  0.399783\n",
       "5  1011010801  265.786340         0.121651  0.430013"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_splits = 3\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Define parameter lists\n",
    "n_estimators_list = [1000, 5000, 10000]\n",
    "learning_rate_list = [0.01, 0.005, 0.001, 0.0005]\n",
    "max_depth_list = [3, 5]\n",
    "subsample_list = [0.5, 0.75]\n",
    "colsample_bytree_list = [0.5, 1.0]\n",
    "early_stopping_rounds_list = [100, 200, 400]\n",
    "\n",
    "# Calculate the total number of combinations\n",
    "total_combinations = (\n",
    "    len(n_estimators_list) *\n",
    "    len(learning_rate_list) *\n",
    "    len(max_depth_list) *\n",
    "    len(subsample_list) *\n",
    "    len(colsample_bytree_list) *\n",
    "    len(early_stopping_rounds_list)\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# categories = [1011010101]\n",
    "categories = df_agg['Category Code'].unique()\n",
    "category_best_params = {}\n",
    "category_best_score = {'Category': [],\n",
    "                       'RMSE': [],\n",
    "                       'Normalized RMSE': [],\n",
    "                       'MAPE': []}\n",
    "for category in categories:\n",
    "    print('\\n######################################################')\n",
    "    print(f'################ CATEGORY: {category} ################')\n",
    "    print('######################################################\\n')\n",
    "\n",
    "    df_category = df_agg[df_agg['Category Code'] == category]\n",
    "\n",
    "    # Feature Engineering\n",
    "    df_final = category_feature_engineering(df_category = df_category)\n",
    "\n",
    "    # Create a progress bar for the overall progress\n",
    "    overall_progress_bar = tqdm(total=total_combinations, desc='Overall Progress', unit='combination')\n",
    "    \n",
    "    # Loop all parameters\n",
    "    best_params = {}\n",
    "    best_score = None\n",
    "    for n_estimators in n_estimators_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for max_depth in max_depth_list:\n",
    "                for subsample in subsample_list:\n",
    "                    for colsample_bytree in colsample_bytree_list:\n",
    "                        for early_stopping_rounds in early_stopping_rounds_list:\n",
    "                             \n",
    "                            # Set up XGBRegressor\n",
    "                            xgb_r = xgb.XGBRegressor(\n",
    "                                n_estimators = n_estimators,\n",
    "                                eval_metrics = mean_squared_error,\n",
    "                                learning_rate = learning_rate,\n",
    "                                max_depth = max_depth,\n",
    "                                subsample = subsample,\n",
    "                                colsample_bytree = colsample_bytree,\n",
    "                                early_stopping_rounds = early_stopping_rounds,\n",
    "                                random_state = 244\n",
    "                            )\n",
    "\n",
    "                            # Define X, y\n",
    "                            X = df_final.drop('Sales', axis = 1)\n",
    "                            y = df_final[['Sales']]\n",
    "\n",
    "                            rmse_list = []\n",
    "                            normalized_rmse_list = []\n",
    "                            mape_list = []\n",
    "                            # Cross Validate\n",
    "                            for train_index, test_index in tscv.split(X):\n",
    "                                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                                # Scale X\n",
    "                                X_train[X_train.columns] = scaler.fit_transform(X_train)\n",
    "                                X_test[X_test.columns] = scaler.transform(X_test)\n",
    "\n",
    "                                # Fit the model\n",
    "                                xgb_r.fit(\n",
    "                                    X = X_train,\n",
    "                                    y = y_train,\n",
    "                                    eval_set = [(X_train, y_train), (X_test, y_test)],\n",
    "                                    verbose = False\n",
    "                                )\n",
    "\n",
    "                                # Predict\n",
    "                                y_pred = xgb_r.predict(X_test)\n",
    "\n",
    "                                # Calculate score\n",
    "                                rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "                                normalized_rmse = rmse / (float(y_test.max()) - float(y_test.min()))\n",
    "                                mape = mean_absolute_percentage_error(y_true = y_test, y_pred = y_pred)\n",
    "\n",
    "                                # Store RMSE for each fold\n",
    "                                rmse_list.append(rmse)\n",
    "                                normalized_rmse_list.append(normalized_rmse)\n",
    "                                mape_list.append(mape)\n",
    "\n",
    "                            \n",
    "                            new_rmse = sum(rmse_list) / len(rmse_list)\n",
    "                            new_normalized_rmse = sum(normalized_rmse_list) / len(normalized_rmse_list)\n",
    "                            new_mape = sum(mape_list) / len(mape_list)\n",
    "                            # print(f'n_estimators: {n_estimators} | learning_rate: {learning_rate} | max_depth: {max_depth} | subsample: {subsample} | colsample_bytree: {colsample_bytree} | early_stopping_rounds: {early_stopping_rounds} >>> RMSE: {new_rmse:.2f} | Normalized RMSE: {new_normalized_rmse:.2f} | MAPE: {new_mape:.2f} %')\n",
    "                            if best_score is None:\n",
    "                                best_score = new_rmse\n",
    "                                best_normalized_rmse = new_normalized_rmse\n",
    "                                best_mape = new_mape\n",
    "\n",
    "                                # Update best_params\n",
    "                                best_params['n_estimators'] = n_estimators\n",
    "                                best_params['learning_rate'] = learning_rate\n",
    "                                best_params['max_depth'] = max_depth\n",
    "                                best_params['subsample'] = subsample\n",
    "                                best_params['colsample_bytree'] = colsample_bytree\n",
    "                                best_params['early_stopping_rounds'] = early_stopping_rounds\n",
    "                            elif new_rmse < best_score:\n",
    "                                best_score = new_rmse\n",
    "                                best_normalized_rmse = new_normalized_rmse\n",
    "                                best_mape = new_mape\n",
    "\n",
    "                                # Update best_params\n",
    "                                best_params['n_estimators'] = n_estimators\n",
    "                                best_params['learning_rate'] = learning_rate\n",
    "                                best_params['max_depth'] = max_depth\n",
    "                                best_params['subsample'] = subsample\n",
    "                                best_params['colsample_bytree'] = colsample_bytree\n",
    "                                best_params['early_stopping_rounds'] = early_stopping_rounds\n",
    "                            \n",
    "                            # Update the overall progress bar\n",
    "                            overall_progress_bar.update(1)\n",
    "\n",
    "    # Close the overall progress bar\n",
    "    overall_progress_bar.close()\n",
    "\n",
    "    # Print the best parameters and corresponding mean squared error\n",
    "    print(f'\\nCategory {category} Best Parameters: {best_params}')\n",
    "    print(f'Category {category} Best Score: {best_score}')\n",
    "\n",
    "    # Store best_params for each category\n",
    "    category_best_params[category] = best_params\n",
    "    category_best_score['Category'].append(category)\n",
    "    category_best_score['RMSE'].append(best_score)\n",
    "    category_best_score['Normalized RMSE'].append(best_normalized_rmse)\n",
    "    category_best_score['MAPE'].append(best_mape)\n",
    "\n",
    "# Summarize\n",
    "print('\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SUMMARY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "display(pd.DataFrame(category_best_params))\n",
    "display(pd.DataFrame(category_best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize\n",
    "print('\\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> SUMMARY <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
    "display(pd.DataFrame(category_best_params))\n",
    "display(pd.DataFrame(category_best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
